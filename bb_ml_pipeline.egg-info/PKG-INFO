Metadata-Version: 2.1
Name: bb-ml-pipeline
Version: 0.1.0
Summary: A pipeline for training, unboxing, and predicting LGBM models
Home-page: https://github.com/yourusername/bb_ml_pipeline
Author: BB ML Pipeline Team
Author-email: example@example.com
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.10
Description-Content-Type: text/markdown

# BB ML Pipeline

A machine learning pipeline for training, unboxing, and predicting LGBM classification models.

## Features

- Data loading from CSV sources
- Metadata-driven data preprocessing (encoding, scaling, imputation)
- Automated model training with k-fold cross-validation
- Hyperparameter optimization with Optuna
- Feature importance extraction using SHAP or TreeInterpreter
- Model prediction with deployment-ready artifacts
- Comprehensive output including predictions, transformed data, and feature importance

## Installation

### Requirements

- Python 3.10+
- Dependencies listed in requirements.txt

### Installation Steps

```bash
# Clone the repository
git clone https://github.com/yourusername/bb_ml_pipeline.git
cd bb_ml_pipeline

# Create and activate a virtual environment (optional but recommended)
python -m venv .venv
source .venv/bin/activate  # On Windows, use: .venv\Scripts\activate

# Install the package
pip install -e .
```

## Usage

The ML Pipeline can be used either as a Python library or via the command line interface.

### Python Library

```python
from bb_ml_pipeline.ml_pipeline import ML_Pipeline

# Initialize the pipeline with configuration files
pipeline = ML_Pipeline(
    modeling_config="path/to/model_config.json",
    data_config="path/to/data_config.json"
)

# Run the pipeline
results = pipeline.run()

# Access results
print(f"Results saved to: {results['output_dir']}")
```

### Command Line Interface

```bash
# Training mode
bb_ml_pipeline --modeling-config path/to/model_config.json --data-config path/to/data_config_train.json

# Prediction mode
bb_ml_pipeline --modeling-config path/to/model_config.json --data-config path/to/data_config_test.json
```

## Configuration Files

### Modeling Configuration (model_config.json)

```json
{
  "target_variable": "target",
  "problem_type": "classification",
  "evaluation_metric": "rocauc",
  "better_performance": "gt",
  "models": ["lgbm"],
  "use_custom_splits": false,
  "retrain_with_whole_dataset": true,
  "sample_for_contribution": 1.0,
  "importance_extraction_method": "shap",
  "num_hp_searches": 5,
  "hyperparameter_space": {
    "lgbm": {
      "boosting_type": "trial.suggest_categorical('boosting_type', ['gbdt'])",
      "learning_rate": "trial.suggest_loguniform('learning_rate', 0.01, 0.2)",
      "num_leaves": "int(trial.suggest_discrete_uniform('num_leaves', 30, 150, 1))",
      "feature_fraction": "trial.suggest_uniform('feature_fraction', 0.1, 1.0)",
      "reg_lambda": "trial.suggest_uniform('reg_lambda', 0.0, 0.1)",
      "n_estimators": "int(trial.suggest_discrete_uniform('n_estimators', 100, 200, 1))",
      "class_weight": "trial.suggest_categorical('class_weight', [None, 'balanced'])",
      "min_data_in_leaf": "int(trial.suggest_discrete_uniform('min_data_in_leaf', 10, 1000, 5))"
    }
  }
}
```

### Data Configuration for Training (data_config_train.json)

```json
{
  "training": true,
  "metadata_file_path": "/path/to/feature_metadata.xlsx",
  "input_data_source": "CSV",
  "file_path": "/path/to/training_data.csv"
}
```

### Data Configuration for Prediction (data_config_test.json)

```json
{
  "training": false,
  "metadata_file_path": "/path/to/feature_metadata.xlsx",
  "input_data_source": "CSV",
  "file_path": "/path/to/test_data.csv",
  "data_processor_file_path": "/path/to/trained_model_dir/deep_insights_input/data_processor.pkl",
  "model_file_path": "/path/to/trained_model_dir/target_classification_lgbm.pkl",
  "uid_column_name": "id_col",
  "predictions_output_path": "/path/to/output/predictions.csv",
  "calculate_importance": true
}
```

### Feature Metadata Format (Excel)

The feature metadata Excel file should have the following structure:

- First row: Feature types (e.g., "float", "category", "bool")
- Second row: Feature usage (0 = don't use, 1 = use)
- Third row: Imputation method (e.g., "mean", "median", "unknown", "most_frequent")
- Fourth row: Category transformation (e.g., "one-hot")
- Fifth row: Super category (optional grouping of features)

## Output Files

The pipeline creates a timestamped output directory containing:

### For Training Mode

- Pickled model file
- Pickled data processor
- Training configuration (JSON)
- Original data with predictions (CSV)
- Transformed data with predictions (CSV)
- Feature importance files (CSV)
- SHAP/TreeInterpreter values (CSV)

### For Prediction Mode

- Predictions file (CSV)
- Original data with predictions (CSV)
- Transformed data with predictions (CSV)
- Feature importance files (if requested)

## License

MIT

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
